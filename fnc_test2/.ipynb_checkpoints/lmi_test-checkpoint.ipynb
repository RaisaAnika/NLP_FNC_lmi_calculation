{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import math\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "BASE_DIR = os.path.dirname(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = join(BASE_DIR, 'fnc.test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 20\n",
    "MIN_FREQ = 5\n",
    "NGRAM = 2\n",
    "#Tokenize Words\n",
    "def get_single_stopwords(dataset, ngram=1):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     fp = open(dataset, \"r\", encoding='utf-8')\n",
    "#     reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    for index, row in df_train.iterrows():\n",
    "        claim = row['Headline']\n",
    "\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "       \n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "\n",
    "    counter = Counter(global_word_counter)\n",
    "    stop_words = counter.most_common(10)\n",
    "    print (sum(counter.values()))\n",
    "    stop_words = [word[0] for word in stop_words]\n",
    "    print (stop_words)\n",
    "    \n",
    "    return stop_words\n",
    "#Process Words in files tokenize them and check if they exist in the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counters(dataset, ngram=NGRAM):\n",
    "\n",
    "    stop_words = get_single_stopwords(test_file, ngram=1)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     fp = open(dataset, \"r\", encoding='utf-8')\n",
    "#     reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    global_label_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    label_word_counter = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        label = row['Stance']\n",
    "        claim = row['Headline']\n",
    "        #words = word_tokenize(claim.lower())\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "\n",
    "        bigrams = ngrams(words, NGRAM)\n",
    "        \n",
    "        \"\"\" \n",
    "        for word in words:\n",
    "            global_word_counter[word] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][word] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        for bigram in bigrams:\n",
    "            bigram = ' '.join(bigram)\n",
    "                \n",
    "            global_word_counter[bigram] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][bigram] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "                global_label_counter[label] += 1\n",
    "                label_word_counter[label][phrase] += 1\n",
    "                phrases += 1\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    print ('Total count: ' + str(phrases))\n",
    "    return global_word_counter, label_word_counter, global_label_counter, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ddd3ee4b0b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_global_word_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_label_word_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_global_label_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c0f363123304>\u001b[0m in \u001b[0;36mget_counters\u001b[0;34m(dataset, ngram)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNGRAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_single_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "valid_global_word_counter, valid_label_word_counter, valid_global_label_counter, valid_words = get_counters(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
