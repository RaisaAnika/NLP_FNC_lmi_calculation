{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import math\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "BASE_DIR = os.path.dirname(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_json(\"fnc.test.no-unrel.jsonl\",lines=True)\n",
    "df_test=pd.read_json(\"fnc.test.no-unrel.generated.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED FILE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>agree</td>\n",
       "      <td>Last week, Apple sent out the invites for its ...</td>\n",
       "      <td>EXCLUSIVE: Apple To Unveil The Long-Awaited Re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>discuss</td>\n",
       "      <td>The three Afghanistan National Army officers w...</td>\n",
       "      <td>Found! Missing Afghan Soldiers Spotted Trying ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>discuss</td>\n",
       "      <td>TORONTO ?? Three missing Afghan soldiers were ...</td>\n",
       "      <td>Report: Three missing Afghan soldiers caught a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>agree</td>\n",
       "      <td>\"Did a woman claiming to have not a third brea...</td>\n",
       "      <td>3-Boobed Woman a Fake</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>discuss</td>\n",
       "      <td>ISIS is using blood money from harvesting orga...</td>\n",
       "      <td>ISIS might be harvesting organs, Iraqi ambassa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gold_label                                           evidence  \\\n",
       "0      agree  Last week, Apple sent out the invites for its ...   \n",
       "1    discuss  The three Afghanistan National Army officers w...   \n",
       "2    discuss  TORONTO ?? Three missing Afghan soldiers were ...   \n",
       "3      agree  \"Did a woman claiming to have not a third brea...   \n",
       "4    discuss  ISIS is using blood money from harvesting orga...   \n",
       "\n",
       "                                               claim  id  \n",
       "0  EXCLUSIVE: Apple To Unveil The Long-Awaited Re...   0  \n",
       "1  Found! Missing Afghan Soldiers Spotted Trying ...   1  \n",
       "2  Report: Three missing Afghan soldiers caught a...   2  \n",
       "3                              3-Boobed Woman a Fake   3  \n",
       "4  ISIS might be harvesting organs, Iraqi ambassa...   4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"GENERATED FILE\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT GENERATED FILE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>agree</td>\n",
       "      <td>Last week, Apple sent out the invites for its ...</td>\n",
       "      <td>EXCLUSIVE: Apple To Unveil The Long-Awaited Re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>discuss</td>\n",
       "      <td>The three Afghanistan National Army officers w...</td>\n",
       "      <td>Found! Missing Afghan Soldiers Spotted Trying ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>discuss</td>\n",
       "      <td>TORONTO – Three missing Afghan soldiers were t...</td>\n",
       "      <td>Report: Three missing Afghan soldiers caught a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>agree</td>\n",
       "      <td>Did a woman claiming to have a third breast pl...</td>\n",
       "      <td>3-Boobed Woman a Fake</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>discuss</td>\n",
       "      <td>ISIS is using blood money from harvesting orga...</td>\n",
       "      <td>ISIS might be harvesting organs, Iraqi ambassa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gold_label                                           evidence  \\\n",
       "0      agree  Last week, Apple sent out the invites for its ...   \n",
       "1    discuss  The three Afghanistan National Army officers w...   \n",
       "2    discuss  TORONTO – Three missing Afghan soldiers were t...   \n",
       "3      agree  Did a woman claiming to have a third breast pl...   \n",
       "4    discuss  ISIS is using blood money from harvesting orga...   \n",
       "\n",
       "                                               claim  id  \n",
       "0  EXCLUSIVE: Apple To Unveil The Long-Awaited Re...   0  \n",
       "1  Found! Missing Afghan Soldiers Spotted Trying ...   1  \n",
       "2  Report: Three missing Afghan soldiers caught a...   2  \n",
       "3                              3-Boobed Woman a Fake   3  \n",
       "4  ISIS might be harvesting organs, Iraqi ambassa...   4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"NOT GENERATED FILE\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 20\n",
    "MIN_FREQ = 5\n",
    "NGRAM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Words\n",
    "def get_single_stopwords(dataset, ngram=1):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "       \n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "\n",
    "    counter = Counter(global_word_counter)\n",
    "    stop_words = counter.most_common(10)\n",
    "    print (sum(counter.values()))\n",
    "    stop_words = [word[0] for word in stop_words]\n",
    "    print (stop_words)\n",
    "    \n",
    "    return stop_words\n",
    "#Process Words in files tokenize them and check if they exist in the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = join(BASE_DIR, 'fnc.test.no-unrel.jsonl')\n",
    "train_file = join(BASE_DIR, 'fnc.test.no-unrel.generated.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Words in files tokenize them and check if they exist in the stop words\n",
    "def get_counters(dataset, ngram=NGRAM):\n",
    "\n",
    "    stop_words = get_single_stopwords(train_file, ngram=1)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    global_label_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    label_word_counter = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        label = dictionary['gold_label']\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        #words = word_tokenize(claim.lower())\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "\n",
    "        bigrams = ngrams(words, NGRAM)\n",
    "        \n",
    "        \"\"\" \n",
    "        for word in words:\n",
    "            global_word_counter[word] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][word] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        for bigram in bigrams:\n",
    "            bigram = ' '.join(bigram)\n",
    "                \n",
    "            global_word_counter[bigram] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][bigram] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "                global_label_counter[label] += 1\n",
    "                label_word_counter[label][phrase] += 1\n",
    "                phrases += 1\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    print ('Total count: ' + str(phrases))\n",
    "    return global_word_counter, label_word_counter, global_label_counter, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6873it [00:00, 31605.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the file: fnc.test.jsonl as valid_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7246it [00:00, 32575.08it/s]\n",
      "2289it [00:00, 21372.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84644\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'woman', 'is', 'isis', 'selfie']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7064it [00:00, 22738.31it/s]\n",
      "7246it [00:00, 41522.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 63019\n",
      "For the file: fnc.test.generated.jsonl as train_file\n",
      "84644\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'woman', 'is', 'isis', 'selfie']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7246it [00:00, 25556.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 65065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For the file: fnc.test.jsonl as valid_file\")\n",
    "valid_global_word_counter, valid_label_word_counter, valid_global_label_counter, valid_words = get_counters(valid_file)\n",
    "print(\"For the file: fnc.test.generated.jsonl as train_file\")\n",
    "train_global_word_counter, train_label_word_counter, train_global_label_counter, train_words = get_counters(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {'agree': [], 'disagree': [], 'discuss': []} #'unrelated': [],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65065\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- agree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "justin bieber        |    1668 |  0.54 |    1663 |  0.55\n",
      "year old             |    1521 |  0.83 |    1556 |  0.88\n",
      "argentina president  |    1334 |  0.48 |    1211 |  0.47\n",
      "into werewolf        |    1223 |  0.55 |    1097 |  0.57\n",
      "bear attack          |    1016 |  0.54 |    1008 |  0.54\n",
      "bieber ringtone      |     967 |  0.55 |     946 |  0.56\n",
      "president adopts     |     964 |  0.66 |    1024 |  0.66\n",
      "adopts jewish        |     936 |  0.62 |     997 |  0.62\n",
      "after he             |     904 |  0.74 |     920 |  0.77\n",
      "wakes up             |     848 |   0.7 |     900 |   0.7\n",
      "turning into         |     834 |  0.54 |     707 |  0.56\n",
      "jewish boy           |     816 |   0.6 |     759 |  0.63\n",
      "from bear            |     785 |  0.55 |     805 |  0.55\n",
      "him from             |     760 |  0.56 |     734 |  0.57\n",
      "jewish godson        |     747 |  0.58 |     719 |   0.6\n",
      "eye after            |     742 |   0.9 |     782 |   0.9\n",
      "zombie cat           |     739 |   1.0 |     778 |   1.0\n",
      "brain surgery        |     711 |   0.7 |     754 |   0.7\n",
      "five days            |     701 |   1.0 |     738 |   1.0\n",
      "birthday party       |     652 |  0.95 |     678 |   1.0\n",
      "65065\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- discuss\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "joan rivers          |    3769 |  0.91 |    3591 |  0.91\n",
      "rivers doctor        |    1780 |  0.88 |    1686 |  0.88\n",
      "steven sotloff       |    1598 |   0.9 |    1520 |   0.9\n",
      "doctor took          |    1420 |  0.94 |    1362 |  0.94\n",
      "islamic state        |    1282 |  0.91 |    1222 |  0.91\n",
      "american journalist  |     912 |  0.96 |     878 |  0.96\n",
      "apple watch          |     907 |  0.94 |     870 |  0.94\n",
      "afghan soldiers      |     781 |  0.87 |     736 |  0.87\n",
      "was under            |     731 |  0.89 |     693 |  0.89\n",
      "under anesthesia     |     731 |  0.89 |     693 |  0.89\n",
      "into two             |     727 |  0.86 |     685 |  0.86\n",
      "journalist steven    |     718 |  0.94 |     689 |  0.94\n",
      "beheading u          |     709 |  0.95 |     681 |  0.95\n",
      "before her           |     705 |   1.0 |     683 |   1.0\n",
      "bakr al              |     690 |   1.0 |     667 |   1.0\n",
      "al baghdadi          |     690 |   1.0 |     667 |   1.0\n",
      "abu bakr             |     690 |   1.0 |     667 |   1.0\n",
      "canadian border      |     662 |  0.91 |     630 |  0.91\n",
      "source joan          |     650 |   1.0 |     630 |   1.0\n",
      "doctor snapped       |     647 |   0.9 |     615 |   0.9\n",
      "65065\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- disagree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "argentina president  |    2115 |  0.33 |    1781 |   0.3\n",
      "third breast         |    1866 |  0.28 |    1729 |  0.26\n",
      "into werewolf        |    1470 |  0.36 |    1071 |  0.32\n",
      "adopt jewish         |    1285 |  0.46 |     918 |  0.44\n",
      "turning into         |    1012 |  0.35 |     631 |   0.3\n",
      "surgery add          |     998 |  0.38 |     878 |  0.36\n",
      "does not             |     965 |   0.5 |     nan |     0\n",
      "jewish child         |     832 |  0.62 |     741 |  0.67\n",
      "not adopt            |     825 |  0.55 |     345 |  0.69\n",
      "didn t               |     805 |  0.58 |     893 |  0.58\n",
      "add third            |     800 |  0.29 |     688 |  0.27\n",
      "president didn       |     786 |  0.63 |     818 |  0.64\n",
      "jewish boy           |     720 |  0.34 |     502 |  0.29\n",
      "stop him             |     701 |  0.36 |     471 |  0.31\n",
      "him turning          |     689 |  0.39 |     448 |  0.34\n",
      "has surgery          |     669 |  0.42 |     754 |  0.42\n",
      "gets third           |     632 |  0.39 |     613 |  0.38\n",
      "florida gets         |     627 |   0.4 |     631 |  0.39\n",
      "has not              |     597 |  0.48 |      65 |  0.38\n",
      "did not              |     592 |  0.45 |     273 |   0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikaraisachowdhury/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#Calculate PMI PL value \n",
    "for label in train_label_word_counter.keys():\n",
    "    words = []\n",
    "    scores = []\n",
    "    pmis = []\n",
    "    valid_pmis = []\n",
    "    valid_scores = []\n",
    "    freqs = []\n",
    "    valid_freqs = []\n",
    "    p_l_train = train_global_label_counter[label] / train_words #generated\n",
    "    p_l_valid = valid_global_label_counter[label] / valid_words #not generated\n",
    "    print (train_words)\n",
    "\n",
    "    word_counter = train_label_word_counter[label]\n",
    "    for w in word_counter:\n",
    "        if train_global_word_counter[w] < MIN_FREQ:\n",
    "            continue\n",
    "\n",
    "        # p(label | word)\n",
    "        score = word_counter[w] / train_global_word_counter[w]\n",
    "        pmi = math.log(score / p_l_train) #pmi is for generated file\n",
    "        #pmi = max(0, pmi)\n",
    "\n",
    "        if w in valid_global_word_counter:\n",
    "            valid_score = valid_label_word_counter[label][w] / valid_global_word_counter[w]\n",
    "            if valid_score == 0:\n",
    "                valid_pmi = float('inf')\n",
    "            else:\n",
    "                valid_pmi = math.log(valid_score / p_l_valid)\n",
    "                #valid_pmi = max(0, math.log(valid_score / p_l_valid))\n",
    "        else:\n",
    "            valid_score = 0\n",
    "            valid_pmi = float('inf')\n",
    "\n",
    "        words.append(w)\n",
    "        scores.append(score)\n",
    "        pmis.append(pmi)\n",
    "        freqs.append(word_counter[w])\n",
    "        valid_freqs.append(valid_label_word_counter[label][w])\n",
    "        valid_scores.append(valid_score)\n",
    "        valid_pmis.append(valid_pmi)\n",
    "    assert(len(words) == len(scores) == len(freqs) == len(valid_freqs) == len(valid_scores) == len(pmis))\n",
    "    \n",
    "    pmis_x_freq = list(np.array(pmis)*freqs/train_words)\n",
    "    valid_pmis_x_freq = list(np.array(valid_pmis)*valid_freqs/valid_words)\n",
    "    pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs = (list(t) for t in zip(*sorted(zip(pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs), reverse=True)))\n",
    "\n",
    "    print(\"\")\n",
    "    print (\"lmi is for generated file and vali_lmi is for non-generated\")\n",
    "    print(\"---- {}\".format(label))\n",
    "    print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format('word', 'lmi', 'p(l|w)', 'valid_lmi', 'valid_p(l|w)'))\n",
    "\n",
    "    #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format('word', 'score', 'pmi', 'lmi', 'freq', 'valid score', 'valid_pmi', 'valid_lmi', 'valid freq'))\n",
    "    print (\"-\"*80)\n",
    "\n",
    "    #filepath = 'top_20_lmi_p_2_' + label + '.csv'\n",
    "    filepath = 'top_1000_unigram_' + label + '.csv'\n",
    "    with open(filepath, 'w') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(min(TOP_N, len(words))):\n",
    "            #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format(words[i], round(scores[i], 3), round(pmis[i],3), round(pmis_x_freq[i],3), freqs[i], round(valid_scores[i],3), round(valid_pmis[i],3), round(valid_pmis_x_freq[i],3), valid_freqs[i]))\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "\n",
    "        '''\n",
    "        extra_words = ['did not', 'yet to', 'does not', 'refused to', 'failed to', 'unable to', 'incapable being', 'united states', 'least one', 'at least', 'person who', 'stars actor', 'least one', 'won award', 'played for']\n",
    "        #extra_words = ['at least one']\n",
    "        for w in extra_words:\n",
    "            i = words.index(w)\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "        '''\n",
    "    \n",
    "    \n",
    "#     limits = [10, 20, 50, 100, 200, 500, 1000]\n",
    "#     #corr_filepath = 'corr_unigram_1000.pkl'\n",
    "#     corr_ind = []\n",
    "#     for limit in limits:\n",
    "#         pears = pearsonr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"pearson correlation for top {}: {} (p-value: {})\".format(limit, round(pears[0],3), round(pears[1],3)))\n",
    "#         corr_ind.append(round(pears[0],3))\n",
    "\n",
    "#         spear = spearmanr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"spearman correlation for top {}: {} (p-value: {})\".format(limit, round(spear[0],3), round(spear[1],3)))\n",
    "\n",
    "#     corr[label] = [limits, corr_ind]\n",
    "\n",
    "    #pickle.dump(corr, open(corr_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Statement:\n",
      "\n",
      " fnc.test.no-unrel.jsonl has a LOWER lmi than fnc.test.no-unrel.generated.jsonl \n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Statement:\")\n",
    "print(\"\")\n",
    "print(\" fnc.test.no-unrel.jsonl has a LOWER lmi than fnc.test.no-unrel.generated.jsonl \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
