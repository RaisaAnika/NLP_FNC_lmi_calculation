{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import math\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "BASE_DIR = os.path.dirname(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = join(BASE_DIR, 'fnc.test.generated.jsonl')\n",
    "valid_file = join(BASE_DIR, 'fnc.test.no-unrel.generated.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 20\n",
    "MIN_FREQ = 5\n",
    "NGRAM = 2\n",
    "\n",
    "#Tokenize Words\n",
    "def get_single_stopwords(dataset, ngram=1):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "       \n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "\n",
    "    counter = Counter(global_word_counter)\n",
    "    stop_words = counter.most_common(10)\n",
    "    print (sum(counter.values()))\n",
    "    stop_words = [word[0] for word in stop_words]\n",
    "    print (stop_words)\n",
    "    \n",
    "    return stop_words\n",
    "#Process Words in files tokenize them and check if they exist in the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Words in files tokenize them and check if they exist in the stop words\n",
    "def get_counters(dataset, ngram=NGRAM):\n",
    "\n",
    "    stop_words = get_single_stopwords(train_file, ngram=1)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    global_label_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    label_word_counter = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        label = dictionary['gold_label']\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        #words = word_tokenize(claim.lower())\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "\n",
    "        bigrams = ngrams(words, NGRAM)\n",
    "        \n",
    "        \"\"\" \n",
    "        for word in words:\n",
    "            global_word_counter[word] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][word] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        for bigram in bigrams:\n",
    "            bigram = ' '.join(bigram)\n",
    "                \n",
    "            global_word_counter[bigram] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][bigram] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "                global_label_counter[label] += 1\n",
    "                label_word_counter[label][phrase] += 1\n",
    "                phrases += 1\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    print ('Total count: ' + str(phrases))\n",
    "    return global_word_counter, label_word_counter, global_label_counter, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1295it [00:00, 12945.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the file: fnc.test.jsonl as valid_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25595it [00:01, 18981.12it/s]\n",
      "1122it [00:00, 11216.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297562\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'isis', 'woman', 'is', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7246it [00:00, 11233.18it/s]\n",
      "2260it [00:00, 16404.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 65098\n",
      "For the file: fnc.test.no-unrel.jsonl as train_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25595it [00:02, 11544.24it/s]\n",
      "454it [00:00, 4534.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297562\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'isis', 'woman', 'is', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25595it [00:02, 10058.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 229538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For the file: fnc.test.jsonl as valid_file\")\n",
    "valid_global_word_counter, valid_label_word_counter, valid_global_label_counter, valid_words = get_counters(valid_file)\n",
    "print(\"For the file: fnc.test.no-unrel.jsonl as train_file\")\n",
    "train_global_word_counter, train_label_word_counter, train_global_label_counter, train_words = get_counters(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {'agree': [], 'disagree': [], 'discuss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229538\n",
      "\n",
      " valid == fnc.test.no-unrel.generated.jsonl & first left == fnc.test.generated.jsonl\n",
      "---- unrelated\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "apple watch          |     248 |   0.8 |     nan |   0.0\n",
      "over ransom          |     123 |  0.83 |     nan |   0.0\n",
      "james foley          |     120 |  0.76 |     nan |   0.0\n",
      "ringtone saves       |     116 |  0.88 |     nan |   0.0\n",
      "saves russian        |     107 |  0.81 |     nan |   0.0\n",
      "cardiac arrest       |     106 |  0.78 |     nan |   0.0\n",
      "air strike           |     104 |  0.81 |     nan |   0.0\n",
      "bill cosby           |      96 |  0.81 |     nan |   0.0\n",
      "adopts godson        |      94 |  0.94 |     nan |   0.0\n",
      "inch iphone          |      92 |  0.87 |     nan |   0.0\n",
      "rivers while         |      91 |  0.95 |     nan |   0.0\n",
      "jacksonville com     |      91 |  0.95 |     nan |   0.0\n",
      "anesthesia cnn       |      91 |  0.95 |     nan |   0.0\n",
      "way out              |      88 |  0.84 |     nan |   0.0\n",
      "russian man          |      87 |  0.85 |     nan |   0.0\n",
      "took selfie          |      87 |  0.73 |     nan |   0.0\n",
      "while star           |      83 |  0.85 |     nan |   0.0\n",
      "star was             |      83 |  0.85 |     nan |   0.0\n",
      "iraqi officials      |      82 |  0.94 |     nan |   0.0\n",
      "4 inch               |      80 |  0.84 |     nan |   0.0\n",
      "229538\n",
      "\n",
      " valid == fnc.test.no-unrel.generated.jsonl & first left == fnc.test.generated.jsonl\n",
      "---- agree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "argentina president  |     537 |  0.16 |    1360 |  0.48\n",
      "justin bieber        |     508 |  0.16 |    1694 |  0.54\n",
      "into werewolf        |     500 |  0.21 |    1241 |  0.55\n",
      "year old             |     488 |  0.27 |    1535 |  0.83\n",
      "turning into         |     352 |  0.21 |     847 |  0.54\n",
      "adopts jewish        |     338 |  0.21 |     948 |  0.62\n",
      "bieber ringtone      |     328 |  0.18 |     981 |  0.55\n",
      "jewish boy           |     325 |  0.23 |     827 |   0.6\n",
      "does not             |     323 |   0.5 |     346 |   0.5\n",
      "bear attack          |     322 |  0.16 |    1031 |  0.54\n",
      "jewish godson        |     310 |  0.23 |     757 |  0.58\n",
      "president adopts     |     290 |  0.19 |     975 |  0.66\n",
      "after he             |     289 |  0.23 |     913 |  0.74\n",
      "him from             |     276 |  0.19 |     772 |  0.56\n",
      "nephew penis         |     256 |  0.33 |     652 |  0.79\n",
      "zombie cat           |     227 |  0.31 |     745 |   1.0\n",
      "eye after            |     224 |  0.27 |     748 |   0.9\n",
      "came out             |     216 |  0.39 |     581 |  0.94\n",
      "from bear            |     203 |  0.15 |     797 |  0.55\n",
      "wakes up             |     203 |  0.17 |     858 |   0.7\n",
      "229538\n",
      "\n",
      " valid == fnc.test.no-unrel.generated.jsonl & first left == fnc.test.generated.jsonl\n",
      "---- discuss\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "joan rivers          |    1222 |  0.27 |    3716 |  0.91\n",
      "rivers doctor        |     596 |  0.28 |    1702 |  0.89\n",
      "took selfie          |     465 |  0.25 |    1919 |  0.96\n",
      "steven sotloff       |     378 |  0.24 |    1575 |   0.9\n",
      "islamic state        |     370 |  0.26 |    1264 |  0.91\n",
      "doctor took          |     361 |  0.26 |    1401 |  0.94\n",
      "into two             |     337 |  0.31 |     716 |  0.86\n",
      "american journalist  |     295 |  0.29 |     901 |  0.96\n",
      "throat procedure     |     292 |  0.47 |     519 |   1.0\n",
      "two companies        |     286 |  0.34 |     598 |  0.91\n",
      "source joan          |     282 |  0.37 |     643 |   1.0\n",
      "beheading u          |     280 |  0.33 |     700 |  0.95\n",
      "tv star              |     278 |  0.43 |     542 |   1.0\n",
      "rivers doc           |     268 |  0.36 |     512 |   0.9\n",
      "journalist steven    |     253 |   0.3 |     708 |  0.94\n",
      "selfie during        |     249 |   0.3 |     783 |  0.99\n",
      "snapped selfie       |     247 |   0.3 |     637 |   0.9\n",
      "doctor snapped       |     247 |   0.3 |     637 |   0.9\n",
      "afghan soldiers      |     220 |  0.25 |     769 |  0.87\n",
      "u hostage            |     217 |  0.36 |     496 |  0.97\n",
      "229538\n",
      "\n",
      " valid == fnc.test.no-unrel.generated.jsonl & first left == fnc.test.generated.jsonl\n",
      "---- disagree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "argentina president  |     707 |  0.11 |    2123 |  0.33\n",
      "third breast         |     655 |   0.1 |    1874 |  0.28\n",
      "into werewolf        |     514 |  0.13 |    1475 |  0.36\n",
      "does not             |     500 |   0.5 |     968 |   0.5\n",
      "adopt jewish         |     481 |   0.2 |    1289 |  0.46\n",
      "turning into         |     360 |  0.13 |    1015 |  0.35\n",
      "not adopt            |     347 |  0.34 |     827 |  0.55\n",
      "surgery add          |     346 |  0.14 |    1002 |  0.38\n",
      "add third            |     301 |  0.12 |     803 |  0.29\n",
      "jewish child         |     274 |  0.23 |     834 |  0.62\n",
      "has not              |     265 |  0.32 |     599 |  0.48\n",
      "president didn       |     259 |  0.24 |     788 |  0.63\n",
      "jewish boy           |     256 |  0.13 |     723 |  0.34\n",
      "didn t               |     255 |   0.2 |     807 |  0.58\n",
      "him turning          |     253 |  0.16 |     691 |  0.39\n",
      "stop him             |     242 |  0.13 |     703 |  0.36\n",
      "climate change       |     234 |  0.51 |     456 |  0.51\n",
      "3 boobed             |     232 |  0.14 |     574 |  0.31\n",
      "president does       |     232 |   0.5 |     449 |   0.5\n",
      "werewolf at          |     221 |  0.31 |     578 |  0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikaraisachowdhury/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#Calculate PMI PL value \n",
    "for label in train_label_word_counter.keys():\n",
    "    words = []\n",
    "    scores = []\n",
    "    pmis = []\n",
    "    valid_pmis = []\n",
    "    valid_scores = []\n",
    "    freqs = []\n",
    "    valid_freqs = []\n",
    "    p_l_train = train_global_label_counter[label] / train_words #generated\n",
    "    p_l_valid = valid_global_label_counter[label] / valid_words #not generated\n",
    "    print (train_words)\n",
    "\n",
    "    word_counter = train_label_word_counter[label]\n",
    "    for w in word_counter:\n",
    "        if train_global_word_counter[w] < MIN_FREQ:\n",
    "            continue\n",
    "\n",
    "        # p(label | word)\n",
    "        score = word_counter[w] / train_global_word_counter[w]\n",
    "        pmi = math.log(score / p_l_train) #pmi is for generated file\n",
    "        #pmi = max(0, pmi)\n",
    "\n",
    "        if w in valid_global_word_counter:\n",
    "            valid_score = valid_label_word_counter[label][w] / valid_global_word_counter[w]\n",
    "            if valid_score == 0:\n",
    "                valid_pmi = float('inf')\n",
    "            else:\n",
    "                valid_pmi = math.log(valid_score / p_l_valid)\n",
    "                #valid_pmi = max(0, math.log(valid_score / p_l_valid))\n",
    "        else:\n",
    "            valid_score = 0\n",
    "            valid_pmi = float('inf')\n",
    "\n",
    "        words.append(w)\n",
    "        scores.append(score)\n",
    "        pmis.append(pmi)\n",
    "        freqs.append(word_counter[w])\n",
    "        valid_freqs.append(valid_label_word_counter[label][w])\n",
    "        valid_scores.append(valid_score)\n",
    "        valid_pmis.append(valid_pmi)\n",
    "\n",
    "    assert(len(words) == len(scores) == len(freqs) == len(valid_freqs) == len(valid_scores) == len(pmis))\n",
    "\n",
    "    pmis_x_freq = list(np.array(pmis)*freqs/train_words)\n",
    "    valid_pmis_x_freq = list(np.array(valid_pmis)*valid_freqs/valid_words)\n",
    "    pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs = (list(t) for t in zip(*sorted(zip(pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs), reverse=True)))\n",
    "\n",
    "    print(\"\")\n",
    "    #print (\"lmi is for generated file and vali_lmi is for \")\n",
    "#     valid_file = join(BASE_DIR, 'fnc.test.jsonl')\n",
    "# train_file = join(BASE_DIR, 'fnc.test.no-unrel.jsonl')\n",
    "   # print(\"valid value are for the file:'fnc.test.no-unrel.jsonl' & other value on the left are for 'fnc.test.jsonl' \")\n",
    "#    train_file = join(BASE_DIR, 'fnc.test.generated.jsonl')\n",
    "# valid_file = join(BASE_DIR, 'fnc.test.no-unrel.generated.jsonl')\n",
    "    print (\" valid == fnc.test.no-unrel.generated.jsonl & first left == fnc.test.generated.jsonl\")\n",
    "    print(\"---- {}\".format(label))\n",
    "    print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format('word', 'lmi', 'p(l|w)', 'valid_lmi', 'valid_p(l|w)'))\n",
    "\n",
    "    #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format('word', 'score', 'pmi', 'lmi', 'freq', 'valid score', 'valid_pmi', 'valid_lmi', 'valid freq'))\n",
    "    print (\"-\"*80)\n",
    "\n",
    "    #filepath = 'top_20_lmi_p_2_' + label + '.csv'\n",
    "    filepath = 'top_1000_unigram_' + label + '.csv'\n",
    "    with open(filepath, 'w') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(min(TOP_N, len(words))):\n",
    "            #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format(words[i], round(scores[i], 3), round(pmis[i],3), round(pmis_x_freq[i],3), freqs[i], round(valid_scores[i],3), round(valid_pmis[i],3), round(valid_pmis_x_freq[i],3), valid_freqs[i]))\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "\n",
    "        '''\n",
    "        extra_words = ['did not', 'yet to', 'does not', 'refused to', 'failed to', 'unable to', 'incapable being', 'united states', 'least one', 'at least', 'person who', 'stars actor', 'least one', 'won award', 'played for']\n",
    "        #extra_words = ['at least one']\n",
    "        for w in extra_words:\n",
    "            i = words.index(w)\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "        '''\n",
    "    \n",
    "    \n",
    "#     limits = [10, 20, 50, 100, 200, 500, 1000]\n",
    "#     #corr_filepath = 'corr_unigram_1000.pkl'\n",
    "#     corr_ind = []\n",
    "#     for limit in limits:\n",
    "#         pears = pearsonr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"pearson correlation for top {}: {} (p-value: {})\".format(limit, round(pears[0],3), round(pears[1],3)))\n",
    "#         corr_ind.append(round(pears[0],3))\n",
    "\n",
    "#         spear = spearmanr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"spearman correlation for top {}: {} (p-value: {})\".format(limit, round(spear[0],3), round(spear[1],3)))\n",
    "\n",
    "#     corr[label] = [limits, corr_ind]\n",
    "\n",
    "#     pickle.dump(corr, open(corr_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
