{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import math\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "BASE_DIR = os.path.dirname(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_json(\"fnc.test.jsonl\",lines=True)\n",
    "df_test=pd.read_json(\"fnc.test.generated.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gold_label                                           evidence  \\\n",
       "0  unrelated  A RESPECTED senior French police officer inves...   \n",
       "1  unrelated  Dave Morin's social networking company Path is...   \n",
       "2  unrelated  A bereaved Afghan mother took revenge on the T...   \n",
       "3  unrelated  Hewlett-Packard is officially splitting in two...   \n",
       "4  unrelated  An airline passenger headed to Dallas was remo...   \n",
       "\n",
       "                                               claim  id  \n",
       "0  Ferguson riots: Pregnant woman loses eye after...   0  \n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...   1  \n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...   2  \n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...   3  \n",
       "4  Argentina's President Adopts Boy to End Werewo...   4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gold_label                                           evidence  \\\n",
       "0  unrelated  A RESPECTED senior French police officer inves...   \n",
       "1  unrelated  Dave Morin's social networking company Path is...   \n",
       "2  unrelated  A bereaved Afghan mother took revenge on the T...   \n",
       "3  unrelated  Hewlett-Packard is officially splitting in two...   \n",
       "4  unrelated  An airline passenger headed to Dallas was remo...   \n",
       "\n",
       "                                               claim  id  \n",
       "0  Ferguson riots: Pregnant woman loses eye after...   0  \n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...   1  \n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...   2  \n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...   3  \n",
       "4  Argentina's President Adopts Boy to End Werewo...   4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 20\n",
    "MIN_FREQ = 5\n",
    "NGRAM = 2\n",
    "\n",
    "#Tokenize Words\n",
    "def get_single_stopwords(dataset, ngram=1):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "       \n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "\n",
    "    counter = Counter(global_word_counter)\n",
    "    stop_words = counter.most_common(10)\n",
    "    print (sum(counter.values()))\n",
    "    stop_words = [word[0] for word in stop_words]\n",
    "    print (stop_words)\n",
    "    \n",
    "    return stop_words\n",
    "#Process Words in files tokenize them and check if they exist in the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = join(BASE_DIR, 'fnc.test.jsonl')\n",
    "train_file = join(BASE_DIR, 'fnc.test.generated.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Words in files tokenize them and check if they exist in the stop words\n",
    "def get_counters(dataset, ngram=NGRAM):\n",
    "\n",
    "    stop_words = get_single_stopwords(train_file, ngram=1)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    global_label_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    label_word_counter = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        label = dictionary['gold_label']\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        #words = word_tokenize(claim.lower())\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "\n",
    "        bigrams = ngrams(words, NGRAM)\n",
    "        \n",
    "        \"\"\" \n",
    "        for word in words:\n",
    "            global_word_counter[word] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][word] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        for bigram in bigrams:\n",
    "            bigram = ' '.join(bigram)\n",
    "                \n",
    "            global_word_counter[bigram] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][bigram] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "                global_label_counter[label] += 1\n",
    "                label_word_counter[label][phrase] += 1\n",
    "                phrases += 1\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    print ('Total count: ' + str(phrases))\n",
    "    return global_word_counter, label_word_counter, global_label_counter, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3423it [00:00, 34225.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the file: fnc.test.jsonl as valid_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25595it [00:00, 39449.88it/s]\n",
      "4569it [00:00, 22612.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297562\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'isis', 'woman', 'is', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25413it [00:01, 23687.37it/s]\n",
      "7517it [00:00, 38334.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 227506\n",
      "for the file: fnc.test.generated.jsonl as train_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25595it [00:00, 38639.28it/s]\n",
      "6008it [00:00, 29838.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297562\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'isis', 'woman', 'is', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25595it [00:00, 28925.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 229538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For the file: fnc.test.jsonl as valid_file\")\n",
    "valid_global_word_counter, valid_label_word_counter, valid_global_label_counter, valid_words = get_counters(valid_file)\n",
    "print(\"For the file: fnc.test.generated.jsonl as train_file\")\n",
    "train_global_word_counter, train_label_word_counter, train_global_label_counter, train_words = get_counters(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {'agree': [], 'disagree': [], 'discuss': []} #'unrelated': [],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229538\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- unrelated\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "apple watch          |     248 |   0.8 |     230 |   0.8\n",
      "over ransom          |     123 |  0.83 |     117 |  0.83\n",
      "james foley          |     120 |  0.76 |     102 |  0.76\n",
      "ringtone saves       |     116 |  0.88 |     112 |  0.88\n",
      "saves russian        |     107 |  0.81 |     107 |  0.82\n",
      "cardiac arrest       |     106 |  0.78 |      95 |  0.78\n",
      "air strike           |     104 |  0.81 |      98 |  0.81\n",
      "bill cosby           |      96 |  0.81 |      90 |  0.81\n",
      "adopts godson        |      94 |  0.94 |      91 |  0.94\n",
      "inch iphone          |      92 |  0.87 |      89 |  0.87\n",
      "rivers while         |      91 |  0.95 |      88 |  0.95\n",
      "jacksonville com     |      91 |  0.95 |      88 |  0.95\n",
      "anesthesia cnn       |      91 |  0.95 |      88 |  0.95\n",
      "way out              |      88 |  0.84 |      84 |  0.84\n",
      "russian man          |      87 |  0.85 |      83 |  0.85\n",
      "took selfie          |      87 |  0.73 |      57 |  0.73\n",
      "while star           |      83 |  0.85 |      80 |  0.85\n",
      "star was             |      83 |  0.85 |      80 |  0.85\n",
      "iraqi officials      |      82 |  0.94 |      80 |  0.94\n",
      "4 inch               |      80 |  0.84 |      76 |  0.84\n",
      "229538\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- agree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "argentina president  |     537 |  0.16 |     421 |  0.15\n",
      "justin bieber        |     508 |  0.16 |     463 |  0.15\n",
      "into werewolf        |     500 |  0.21 |     369 |  0.18\n",
      "year old             |     488 |  0.27 |     453 |  0.26\n",
      "turning into         |     352 |  0.21 |     233 |  0.17\n",
      "adopts jewish        |     338 |  0.21 |     357 |  0.21\n",
      "bieber ringtone      |     328 |  0.18 |     287 |  0.16\n",
      "jewish boy           |     325 |  0.23 |     252 |   0.2\n",
      "does not             |     323 |   0.5 |     nan |     0\n",
      "bear attack          |     322 |  0.16 |     291 |  0.15\n",
      "jewish godson        |     310 |  0.23 |     264 |  0.21\n",
      "president adopts     |     290 |  0.19 |     309 |  0.19\n",
      "after he             |     289 |  0.23 |     269 |  0.22\n",
      "him from             |     276 |  0.19 |     236 |  0.17\n",
      "nephew penis         |     256 |  0.33 |     235 |  0.32\n",
      "zombie cat           |     227 |  0.31 |     237 |  0.31\n",
      "eye after            |     224 |  0.27 |     235 |  0.27\n",
      "came out             |     216 |  0.39 |     209 |  0.38\n",
      "from bear            |     203 |  0.15 |     198 |  0.14\n",
      "wakes up             |     203 |  0.17 |     217 |  0.17\n",
      "229538\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- discuss\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "joan rivers          |    1222 |  0.27 |    1210 |  0.27\n",
      "rivers doctor        |     596 |  0.28 |     590 |  0.28\n",
      "took selfie          |     465 |  0.25 |     459 |  0.25\n",
      "steven sotloff       |     378 |  0.24 |     371 |  0.24\n",
      "islamic state        |     370 |  0.26 |     366 |  0.26\n",
      "doctor took          |     361 |  0.26 |     357 |  0.26\n",
      "into two             |     337 |  0.31 |     335 |  0.31\n",
      "american journalist  |     295 |  0.29 |     292 |  0.29\n",
      "throat procedure     |     292 |  0.47 |     292 |  0.47\n",
      "two companies        |     286 |  0.34 |     285 |  0.34\n",
      "source joan          |     282 |  0.37 |     282 |  0.37\n",
      "beheading u          |     280 |  0.33 |     279 |  0.33\n",
      "tv star              |     278 |  0.43 |     278 |  0.43\n",
      "rivers doc           |     268 |  0.36 |     267 |  0.36\n",
      "journalist steven    |     253 |   0.3 |     251 |   0.3\n",
      "selfie during        |     249 |   0.3 |     248 |   0.3\n",
      "snapped selfie       |     247 |   0.3 |     245 |   0.3\n",
      "doctor snapped       |     247 |   0.3 |     245 |   0.3\n",
      "afghan soldiers      |     220 |  0.25 |     216 |  0.25\n",
      "u hostage            |     217 |  0.36 |     216 |  0.36\n",
      "229538\n",
      "\n",
      "lmi is for generated file and vali_lmi is for non-generated\n",
      "---- disagree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "argentina president  |     707 |  0.11 |     546 |  0.09\n",
      "third breast         |     655 |   0.1 |     570 |  0.09\n",
      "into werewolf        |     514 |  0.13 |     332 |   0.1\n",
      "does not             |     500 |   0.5 |     nan |     0\n",
      "adopt jewish         |     481 |   0.2 |     297 |  0.16\n",
      "turning into         |     360 |  0.13 |     194 |  0.09\n",
      "not adopt            |     347 |  0.34 |     100 |  0.21\n",
      "surgery add          |     346 |  0.14 |     280 |  0.12\n",
      "add third            |     301 |  0.12 |     242 |   0.1\n",
      "jewish child         |     274 |  0.23 |     214 |   0.2\n",
      "has not              |     265 |  0.32 |      15 |  0.08\n",
      "president didn       |     259 |  0.24 |     257 |  0.23\n",
      "jewish boy           |     256 |  0.13 |     158 |  0.09\n",
      "didn t               |     255 |   0.2 |     277 |   0.2\n",
      "him turning          |     253 |  0.16 |     142 |  0.11\n",
      "stop him             |     242 |  0.13 |     141 |   0.1\n",
      "climate change       |     234 |  0.51 |     209 |  0.52\n",
      "3 boobed             |     232 |  0.14 |     205 |  0.12\n",
      "president does       |     232 |   0.5 |     nan |     0\n",
      "werewolf at          |     221 |  0.31 |     175 |  0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikaraisachowdhury/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#Calculate PMI PL value \n",
    "for label in train_label_word_counter.keys():\n",
    "    words = []\n",
    "    scores = []\n",
    "    pmis = []\n",
    "    valid_pmis = []\n",
    "    valid_scores = []\n",
    "    freqs = []\n",
    "    valid_freqs = []\n",
    "    p_l_train = train_global_label_counter[label] / train_words #generated\n",
    "    p_l_valid = valid_global_label_counter[label] / valid_words #not generated\n",
    "    print (train_words)\n",
    "\n",
    "    word_counter = train_label_word_counter[label]\n",
    "    for w in word_counter:\n",
    "        if train_global_word_counter[w] < MIN_FREQ:\n",
    "            continue\n",
    "\n",
    "        # p(label | word)\n",
    "        score = word_counter[w] / train_global_word_counter[w]\n",
    "        pmi = math.log(score / p_l_train) #pmi is for generated file\n",
    "        #pmi = max(0, pmi)\n",
    "\n",
    "        if w in valid_global_word_counter:\n",
    "            valid_score = valid_label_word_counter[label][w] / valid_global_word_counter[w]\n",
    "            if valid_score == 0:\n",
    "                valid_pmi = float('inf')\n",
    "            else:\n",
    "                valid_pmi = math.log(valid_score / p_l_valid)\n",
    "                #valid_pmi = max(0, math.log(valid_score / p_l_valid))\n",
    "        else:\n",
    "            valid_score = 0\n",
    "            valid_pmi = float('inf')\n",
    "\n",
    "        words.append(w)\n",
    "        scores.append(score)\n",
    "        pmis.append(pmi)\n",
    "        freqs.append(word_counter[w])\n",
    "        valid_freqs.append(valid_label_word_counter[label][w])\n",
    "        valid_scores.append(valid_score)\n",
    "        valid_pmis.append(valid_pmi)\n",
    "\n",
    "    assert(len(words) == len(scores) == len(freqs) == len(valid_freqs) == len(valid_scores) == len(pmis))\n",
    "\n",
    "    pmis_x_freq = list(np.array(pmis)*freqs/train_words)\n",
    "    valid_pmis_x_freq = list(np.array(valid_pmis)*valid_freqs/valid_words)\n",
    "    pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs = (list(t) for t in zip(*sorted(zip(pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs), reverse=True)))\n",
    "\n",
    "    print(\"\")\n",
    "    print (\"lmi is for generated file and vali_lmi is for non-generated\")\n",
    "    print(\"---- {}\".format(label))\n",
    "    print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format('word', 'lmi', 'p(l|w)', 'valid_lmi', 'valid_p(l|w)'))\n",
    "\n",
    "    #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format('word', 'score', 'pmi', 'lmi', 'freq', 'valid score', 'valid_pmi', 'valid_lmi', 'valid freq'))\n",
    "    print (\"-\"*80)\n",
    "\n",
    "    #filepath = 'top_20_lmi_p_2_' + label + '.csv'\n",
    "    filepath = 'top_1000_unigram_' + label + '.csv'\n",
    "    with open(filepath, 'w') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(min(TOP_N, len(words))):\n",
    "            #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format(words[i], round(scores[i], 3), round(pmis[i],3), round(pmis_x_freq[i],3), freqs[i], round(valid_scores[i],3), round(valid_pmis[i],3), round(valid_pmis_x_freq[i],3), valid_freqs[i]))\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "\n",
    "        '''\n",
    "        extra_words = ['did not', 'yet to', 'does not', 'refused to', 'failed to', 'unable to', 'incapable being', 'united states', 'least one', 'at least', 'person who', 'stars actor', 'least one', 'won award', 'played for']\n",
    "        #extra_words = ['at least one']\n",
    "        for w in extra_words:\n",
    "            i = words.index(w)\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "        '''\n",
    "    \n",
    "    \n",
    "#     limits = [10, 20, 50, 100, 200, 500, 1000]\n",
    "#     #corr_filepath = 'corr_unigram_1000.pkl'\n",
    "#     corr_ind = []\n",
    "#     for limit in limits:\n",
    "#         pears = pearsonr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"pearson correlation for top {}: {} (p-value: {})\".format(limit, round(pears[0],3), round(pears[1],3)))\n",
    "#         corr_ind.append(round(pears[0],3))\n",
    "\n",
    "#         spear = spearmanr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"spearman correlation for top {}: {} (p-value: {})\".format(limit, round(spear[0],3), round(spear[1],3)))\n",
    "\n",
    "#     corr[label] = [limits, corr_ind]\n",
    "\n",
    "    #pickle.dump(corr, open(corr_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      "\n",
      " fnc.test.jsonl has a LOWER lmi than fnc.test.generated.jsonl \n"
     ]
    }
   ],
   "source": [
    "print(\"Observation:\")\n",
    "print(\"\")\n",
    "print(\" fnc.test.jsonl has a LOWER lmi than fnc.test.generated.jsonl \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
