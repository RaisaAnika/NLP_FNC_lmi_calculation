{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import math\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "BASE_DIR = os.path.dirname(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train=pd.read_json(\"fnc.test.jsonl\",lines=True)\n",
    "# df_test=pd.read_json(\"fnc.test.no-unrel.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = join(BASE_DIR, 'fnc.test.jsonl')\n",
    "valid_file = join(BASE_DIR, 'fnc.test.no-unrel.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 20\n",
    "MIN_FREQ = 5\n",
    "NGRAM = 2\n",
    "\n",
    "#Tokenize Words\n",
    "def get_single_stopwords(dataset, ngram=1):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "       \n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "\n",
    "    counter = Counter(global_word_counter)\n",
    "    stop_words = counter.most_common(10)\n",
    "    print (sum(counter.values()))\n",
    "    stop_words = [word[0] for word in stop_words]\n",
    "    print (stop_words)\n",
    "    \n",
    "    return stop_words\n",
    "#Process Words in files tokenize them and check if they exist in the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Words in files tokenize them and check if they exist in the stop words\n",
    "def get_counters(dataset, ngram=NGRAM):\n",
    "\n",
    "    stop_words = get_single_stopwords(train_file, ngram=1)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    fp = open(dataset, \"r\", encoding='utf-8')\n",
    "    reader = jsonlines.Reader(fp)\n",
    "\n",
    "    global_word_counter = defaultdict(int)\n",
    "    global_label_counter = defaultdict(int)\n",
    "    phrases = 0\n",
    "\n",
    "    label_word_counter = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "    for dictionary in tqdm(reader.iter()):\n",
    "        label = dictionary['gold_label']\n",
    "        claim = dictionary['claim']\n",
    "\n",
    "        #words = word_tokenize(claim.lower())\n",
    "        words = tokenizer.tokenize(claim.lower())\n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in stop_words]\n",
    "\n",
    "        bigrams = ngrams(words, NGRAM)\n",
    "        \n",
    "        \"\"\" \n",
    "        for word in words:\n",
    "            global_word_counter[word] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][word] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        for bigram in bigrams:\n",
    "            bigram = ' '.join(bigram)\n",
    "                \n",
    "            global_word_counter[bigram] += 1\n",
    "            global_label_counter[label] += 1\n",
    "            label_word_counter[label][bigram] += 1\n",
    "            phrases += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for i  in range(len(words) + 1):\n",
    "            if i > ngram - 1:\n",
    "                phrase = ' '.join(words[i - ngram:i])\n",
    "                global_word_counter[phrase] += 1\n",
    "                global_label_counter[label] += 1\n",
    "                label_word_counter[label][phrase] += 1\n",
    "                phrases += 1\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    print ('Total count: ' + str(phrases))\n",
    "    return global_word_counter, label_word_counter, global_label_counter, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2139it [00:00, 21368.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the file: fnc.test.jsonl as valid_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25413it [00:00, 26259.23it/s]\n",
      "1801it [00:00, 18009.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294988\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'isis', 'is', 'woman', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7064it [00:00, 11206.13it/s]\n",
      "1634it [00:00, 16337.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 63066\n",
      "For the file: fnc.test.no-unrel.jsonl as train_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25413it [00:01, 13010.31it/s]\n",
      "1525it [00:00, 15243.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294988\n",
      "['to', 's', 'a', 'in', 'of', 'the', 'isis', 'is', 'woman', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25413it [00:02, 9646.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 227506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For the file: fnc.test.jsonl as valid_file\")\n",
    "valid_global_word_counter, valid_label_word_counter, valid_global_label_counter, valid_words = get_counters(valid_file)\n",
    "print(\"For the file: fnc.test.no-unrel.jsonl as train_file\")\n",
    "train_global_word_counter, train_label_word_counter, train_global_label_counter, train_words = get_counters(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {'agree': [], 'disagree': [], 'discuss': []} #'unrelated': [],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anikaraisachowdhury/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227506\n",
      "\n",
      "valid value are for the file:'fnc.test.no-unrel.jsonl' & other value on the left are for 'fnc.test.jsonl' \n",
      "---- unrelated\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "apple watch          |     230 |   0.8 |     nan |   0.0\n",
      "over ransom          |     117 |  0.83 |     nan |   0.0\n",
      "ringtone saves       |     112 |  0.88 |     nan |   0.0\n",
      "saves russian        |     107 |  0.82 |     nan |   0.0\n",
      "james foley          |     102 |  0.76 |     nan |   0.0\n",
      "air strike           |      98 |  0.81 |     nan |   0.0\n",
      "cardiac arrest       |      95 |  0.78 |     nan |   0.0\n",
      "adopts godson        |      91 |  0.94 |     nan |   0.0\n",
      "bill cosby           |      90 |  0.81 |     nan |   0.0\n",
      "inch iphone          |      89 |  0.87 |     nan |   0.0\n",
      "rivers while         |      88 |  0.95 |     nan |   0.0\n",
      "jacksonville com     |      88 |  0.95 |     nan |   0.0\n",
      "anesthesia cnn       |      88 |  0.95 |     nan |   0.0\n",
      "way out              |      84 |  0.84 |     nan |   0.0\n",
      "russian man          |      83 |  0.85 |     nan |   0.0\n",
      "iraqi officials      |      80 |  0.94 |     nan |   0.0\n",
      "while star           |      80 |  0.85 |     nan |   0.0\n",
      "star was             |      80 |  0.85 |     nan |   0.0\n",
      "4 inch               |      76 |  0.84 |     nan |   0.0\n",
      "throat surgery       |      75 |  0.91 |     nan |   0.0\n",
      "227506\n",
      "\n",
      "valid value are for the file:'fnc.test.no-unrel.jsonl' & other value on the left are for 'fnc.test.jsonl' \n",
      "---- agree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "justin bieber        |     463 |  0.15 |    1688 |  0.55\n",
      "year old             |     453 |  0.26 |    1569 |  0.88\n",
      "argentina president  |     421 |  0.15 |    1235 |  0.47\n",
      "into werewolf        |     369 |  0.18 |    1112 |  0.57\n",
      "adopts jewish        |     357 |  0.21 |    1010 |  0.62\n",
      "president adopts     |     309 |  0.19 |    1036 |  0.66\n",
      "bear attack          |     291 |  0.15 |    1024 |  0.54\n",
      "bieber ringtone      |     287 |  0.16 |     960 |  0.56\n",
      "after he             |     269 |  0.22 |     929 |  0.77\n",
      "jewish godson        |     264 |  0.21 |     728 |   0.6\n",
      "jewish boy           |     252 |   0.2 |     768 |  0.63\n",
      "zombie cat           |     237 |  0.31 |     783 |   1.0\n",
      "him from             |     236 |  0.17 |     745 |  0.57\n",
      "nephew penis         |     235 |  0.32 |     661 |  0.84\n",
      "eye after            |     235 |  0.27 |     789 |   0.9\n",
      "turning into         |     233 |  0.17 |     718 |  0.56\n",
      "wakes up             |     217 |  0.17 |     909 |   0.7\n",
      "five days            |     209 |  0.28 |     743 |   1.0\n",
      "came out             |     209 |  0.38 |     603 |   1.0\n",
      "from bear            |     198 |  0.14 |     817 |  0.55\n",
      "227506\n",
      "\n",
      "valid value are for the file:'fnc.test.no-unrel.jsonl' & other value on the left are for 'fnc.test.jsonl' \n",
      "---- discuss\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "joan rivers          |    1210 |  0.27 |    3537 |  0.91\n",
      "rivers doctor        |     590 |  0.28 |    1613 |  0.89\n",
      "took selfie          |     459 |  0.25 |    1844 |  0.96\n",
      "steven sotloff       |     371 |  0.24 |    1497 |   0.9\n",
      "islamic state        |     366 |  0.26 |    1204 |  0.91\n",
      "doctor took          |     357 |  0.26 |    1344 |  0.94\n",
      "into two             |     335 |  0.31 |     673 |  0.86\n",
      "american journalist  |     292 |  0.29 |     866 |  0.96\n",
      "throat procedure     |     292 |  0.47 |     502 |   1.0\n",
      "two companies        |     285 |  0.34 |     568 |  0.91\n",
      "source joan          |     282 |  0.37 |     622 |   1.0\n",
      "beheading u          |     279 |  0.33 |     672 |  0.95\n",
      "tv star              |     278 |  0.43 |     525 |   1.0\n",
      "rivers doc           |     267 |  0.36 |     486 |   0.9\n",
      "journalist steven    |     251 |   0.3 |     679 |  0.94\n",
      "selfie during        |     248 |   0.3 |     756 |  0.99\n",
      "snapped selfie       |     245 |   0.3 |     606 |   0.9\n",
      "doctor snapped       |     245 |   0.3 |     606 |   0.9\n",
      "afghan soldiers      |     216 |  0.25 |     724 |  0.87\n",
      "u hostage            |     216 |  0.36 |     478 |  0.97\n",
      "227506\n",
      "\n",
      "valid value are for the file:'fnc.test.no-unrel.jsonl' & other value on the left are for 'fnc.test.jsonl' \n",
      "---- disagree\n",
      "word                 | lmi     | p(l|w) | valid_lmi | valid_p(l|w)\n",
      "--------------------------------------------------------------------------------\n",
      "third breast         |     570 |  0.09 |    1735 |  0.26\n",
      "argentina president  |     546 |  0.09 |    1787 |   0.3\n",
      "into werewolf        |     332 |   0.1 |    1075 |  0.32\n",
      "adopt jewish         |     297 |  0.16 |     920 |  0.44\n",
      "surgery add          |     280 |  0.12 |     881 |  0.36\n",
      "didn t               |     277 |   0.2 |     894 |  0.58\n",
      "president didn       |     257 |  0.23 |     819 |  0.64\n",
      "add third            |     242 |   0.1 |     690 |  0.27\n",
      "has surgery          |     223 |  0.13 |     756 |  0.42\n",
      "jewish child         |     214 |   0.2 |     742 |  0.67\n",
      "climate change       |     209 |  0.52 |     430 |  0.52\n",
      "3 boobed             |     205 |  0.12 |     538 |  0.29\n",
      "florida gets         |     197 |  0.13 |     633 |  0.39\n",
      "turning into         |     194 |  0.09 |     633 |   0.3\n",
      "gets third           |     183 |  0.12 |     614 |  0.38\n",
      "werewolf at          |     175 |  0.27 |     510 |  0.63\n",
      "takes on             |     175 |  0.27 |     510 |  0.63\n",
      "president takes      |     175 |  0.27 |     510 |  0.63\n",
      "on godson            |     175 |  0.27 |     510 |  0.63\n",
      "not keep             |     175 |  0.27 |     510 |  0.63\n"
     ]
    }
   ],
   "source": [
    "#Calculate PMI PL value \n",
    "for label in train_label_word_counter.keys():\n",
    "    words = []\n",
    "    scores = []\n",
    "    pmis = []\n",
    "    valid_pmis = []\n",
    "    valid_scores = []\n",
    "    freqs = []\n",
    "    valid_freqs = []\n",
    "    p_l_train = train_global_label_counter[label] / train_words #generated\n",
    "    p_l_valid = valid_global_label_counter[label] / valid_words #not generated\n",
    "    print (train_words)\n",
    "\n",
    "    word_counter = train_label_word_counter[label]\n",
    "    for w in word_counter:\n",
    "        if train_global_word_counter[w] < MIN_FREQ:\n",
    "            continue\n",
    "\n",
    "        # p(label | word)\n",
    "        score = word_counter[w] / train_global_word_counter[w]\n",
    "        pmi = math.log(score / p_l_train) #pmi is for generated file\n",
    "        #pmi = max(0, pmi)\n",
    "\n",
    "        if w in valid_global_word_counter:\n",
    "            valid_score = valid_label_word_counter[label][w] / valid_global_word_counter[w]\n",
    "            if valid_score == 0:\n",
    "                valid_pmi = float('inf')\n",
    "            else:\n",
    "                valid_pmi = math.log(valid_score / p_l_valid)\n",
    "                #valid_pmi = max(0, math.log(valid_score / p_l_valid))\n",
    "        else:\n",
    "            valid_score = 0\n",
    "            valid_pmi = float('inf')\n",
    "\n",
    "        words.append(w)\n",
    "        scores.append(score)\n",
    "        pmis.append(pmi)\n",
    "        freqs.append(word_counter[w])\n",
    "        valid_freqs.append(valid_label_word_counter[label][w])\n",
    "        valid_scores.append(valid_score)\n",
    "        valid_pmis.append(valid_pmi)\n",
    "\n",
    "    assert(len(words) == len(scores) == len(freqs) == len(valid_freqs) == len(valid_scores) == len(pmis))\n",
    "\n",
    "    pmis_x_freq = list(np.array(pmis)*freqs/train_words)\n",
    "    valid_pmis_x_freq = list(np.array(valid_pmis)*valid_freqs/valid_words)\n",
    "    pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs = (list(t) for t in zip(*sorted(zip(pmis_x_freq, pmis, scores, freqs, words, valid_scores, valid_pmis, valid_pmis_x_freq, valid_freqs), reverse=True)))\n",
    "\n",
    "    print(\"\")\n",
    "    #print (\"lmi is for generated file and vali_lmi is for \")\n",
    "#     valid_file = join(BASE_DIR, 'fnc.test.jsonl')\n",
    "# train_file = join(BASE_DIR, 'fnc.test.no-unrel.jsonl')\n",
    "    print(\"valid value are for the file:'fnc.test.no-unrel.jsonl' & other value on the left are for 'fnc.test.jsonl' \")\n",
    "    print(\"---- {}\".format(label))\n",
    "    print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format('word', 'lmi', 'p(l|w)', 'valid_lmi', 'valid_p(l|w)'))\n",
    "\n",
    "    #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format('word', 'score', 'pmi', 'lmi', 'freq', 'valid score', 'valid_pmi', 'valid_lmi', 'valid freq'))\n",
    "    print (\"-\"*80)\n",
    "\n",
    "    #filepath = 'top_20_lmi_p_2_' + label + '.csv'\n",
    "    filepath = 'top_1000_unigram_' + label + '.csv'\n",
    "    with open(filepath, 'w') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(min(TOP_N, len(words))):\n",
    "            #print(\"{:20} | {:6} | {:7} | {:7} | {:4} | {:11} | {:10} | {:10} | {:10}\".format(words[i], round(scores[i], 3), round(pmis[i],3), round(pmis_x_freq[i],3), freqs[i], round(valid_scores[i],3), round(valid_pmis[i],3), round(valid_pmis_x_freq[i],3), valid_freqs[i]))\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "\n",
    "        '''\n",
    "        extra_words = ['did not', 'yet to', 'does not', 'refused to', 'failed to', 'unable to', 'incapable being', 'united states', 'least one', 'at least', 'person who', 'stars actor', 'least one', 'won award', 'played for']\n",
    "        #extra_words = ['at least one']\n",
    "        for w in extra_words:\n",
    "            i = words.index(w)\n",
    "            if not math.isnan(valid_pmis_x_freq[i]): \n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], int(round(valid_pmis_x_freq[i]*10**6)), round(valid_scores[i],2), valid_freqs[i]])\n",
    "            else:\n",
    "                print(\"{:20} | {:7} | {:5} | {:7} | {:5}\".format(words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), valid_pmis_x_freq[i], round(valid_scores[i],2)))\n",
    "                csv_writer.writerow([words[i], int(round(pmis_x_freq[i]*10**6)), round(scores[i],2), freqs[i], valid_pmis_x_freq[i], round(valid_scores[i],2), valid_freqs[i]])\n",
    "        '''\n",
    "    \n",
    "    \n",
    "#     limits = [10, 20, 50, 100, 200, 500, 1000]\n",
    "#     #corr_filepath = 'corr_unigram_1000.pkl'\n",
    "#     corr_ind = []\n",
    "#     for limit in limits:\n",
    "#         pears = pearsonr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"pearson correlation for top {}: {} (p-value: {})\".format(limit, round(pears[0],3), round(pears[1],3)))\n",
    "#         corr_ind.append(round(pears[0],3))\n",
    "\n",
    "#         spear = spearmanr(scores[0:limit], valid_scores[0:limit])\n",
    "#         print (\"spearman correlation for top {}: {} (p-value: {})\".format(limit, round(spear[0],3), round(spear[1],3)))\n",
    "\n",
    "#     corr[label] = [limits, corr_ind]\n",
    "\n",
    "#     pickle.dump(corr, open(corr_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
